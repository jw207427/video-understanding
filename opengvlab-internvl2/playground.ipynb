{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9ff956f4-9348-4cf4-8ada-43feaa0de125",
   "metadata": {},
   "source": [
    "### Install packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aff6106c-3baa-4f71-a850-4f1c552446e8",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install einops==0.8.0\n",
    "!pip install decord==0.6.0\n",
    "!pip install git+https://github.com/huggingface/transformers@21fac7abba2a37fae86106f87fcf9974fd1e3830\n",
    "!pip install sentencepiece==0.2.0\n",
    "!pip install accelerate==0.34.2\n",
    "!pip install timm==1.0.9"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5b7028b-0186-4988-9eec-0d5ec8074170",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Import modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b366e527-511f-4abe-8b54-c0ffe1d03bc1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import inference.utils as utils\n",
    "import torch\n",
    "from transformers import AutoModel, AutoTokenizer\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5ae7d76-b4bd-4d1e-844f-a79802965181",
   "metadata": {},
   "source": [
    "### Load model on multiple GPUs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddaaa350-a198-485d-9e34-82dbe97c2848",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def split_model(model_name):\n",
    "    device_map = {}\n",
    "    world_size = torch.cuda.device_count()\n",
    "    num_layers = {\n",
    "        'InternVL2-1B': 24, 'InternVL2-2B': 24, 'InternVL2-4B': 32, 'InternVL2-8B': 32,\n",
    "        'InternVL2-26B': 48, 'InternVL2-40B': 60, 'InternVL2-Llama3-76B': 80}[model_name]\n",
    "    # Since the first GPU will be used for ViT, treat it as half a GPU.\n",
    "    num_layers_per_gpu = math.ceil(num_layers / (world_size - 0.5))\n",
    "    num_layers_per_gpu = [num_layers_per_gpu] * world_size\n",
    "    num_layers_per_gpu[0] = math.ceil(num_layers_per_gpu[0] * 0.5)\n",
    "    layer_cnt = 0\n",
    "    for i, num_layer in enumerate(num_layers_per_gpu):\n",
    "        for j in range(num_layer):\n",
    "            device_map[f'language_model.model.layers.{layer_cnt}'] = i\n",
    "            layer_cnt += 1\n",
    "    device_map['vision_model'] = 0\n",
    "    device_map['mlp1'] = 0\n",
    "    device_map['language_model.model.tok_embeddings'] = 0\n",
    "    device_map['language_model.model.embed_tokens'] = 0\n",
    "    device_map['language_model.output'] = 0\n",
    "    device_map['language_model.model.norm'] = 0\n",
    "    device_map['language_model.lm_head'] = 0\n",
    "    device_map[f'language_model.model.layers.{num_layers - 1}'] = 0\n",
    "\n",
    "    return device_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab6cc731-5fee-4d8d-b9d1-b3aae0adccae",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model_name = \"InternVL2-4B\"\n",
    "path = f\"pretrained/{model_name}\"\n",
    "device_map = split_model(model_name)\n",
    "model = AutoModel.from_pretrained(\n",
    "    path,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    low_cpu_mem_usage=True,\n",
    "    use_flash_attn=True,\n",
    "    trust_remote_code=True,\n",
    "    device_map=device_map).eval()\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(path, trust_remote_code=True, use_fast=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9206d598-ccfe-4ceb-b7b9-b402528c8e79",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44c5b70c-0f69-4ec5-a5a2-3048ff851a92",
   "metadata": {},
   "source": [
    "## Load single model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfb737b0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# path = 'pretrained/InternVL2-1B'\n",
    "\n",
    "# model = AutoModel.from_pretrained(\n",
    "#     path,\n",
    "#     torch_dtype=torch.bfloat16,\n",
    "#     low_cpu_mem_usage=True,\n",
    "#     use_flash_attn=True,\n",
    "#     trust_remote_code=True).eval().cuda()\n",
    "# tokenizer = AutoTokenizer.from_pretrained(path, trust_remote_code=True, use_fast=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba258422-112e-4206-9add-f04690d66856",
   "metadata": {},
   "source": [
    "### Test inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57645ecf-7a7a-4c39-a402-fdd0babf4c51",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# set the max number of tiles in `max_num`\n",
    "pixel_values = utils.load_image('./examples/examples_image1.jpg', max_num=12).to(torch.bfloat16).cuda()\n",
    "generation_config = dict(max_new_tokens=1024, do_sample=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4594651d-67d4-4749-b676-53d380bf1dfb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%time\n",
    "# pure-text conversation (纯文本对话)\n",
    "question = 'Hello, who are you?'\n",
    "response, history = model.chat(tokenizer, None, question, generation_config, history=None, return_history=True)\n",
    "print(f'User: {question}\\nAssistant: {response}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c555a9c5-64d3-4431-a91d-0a4d7a93faa8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%time\n",
    "question = 'Can you tell me a story?'\n",
    "response, history = model.chat(tokenizer, None, question, generation_config, history=history, return_history=True)\n",
    "print(f'User: {question}\\nAssistant: {response}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38681cfd-26c5-4b7f-8c16-ffedd0f6d2fc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%time\n",
    "# single-image single-round conversation (单图单轮对话)\n",
    "question = '<image>\\nPlease describe the image shortly.'\n",
    "response = model.chat(tokenizer, pixel_values, question, generation_config, num_patches_list=None)\n",
    "print(f'User: {question}\\nAssistant: {response}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f0ee962-0574-4d1e-a5df-fa485cd112c3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# single-image multi-round conversation (单图多轮对话)\n",
    "question = '<image>\\nPlease describe the image in detail.'\n",
    "response, history = model.chat(tokenizer, pixel_values, question, generation_config, history=None, return_history=True)\n",
    "print(f'User: {question}\\nAssistant: {response}')\n",
    "\n",
    "question = 'Please write a poem according to the image.'\n",
    "response, history = model.chat(tokenizer, pixel_values, question, generation_config, history=history, return_history=True)\n",
    "print(f'User: {question}\\nAssistant: {response}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2244aeda-9cd1-48fc-b6d0-a48f2349b9de",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# multi-image multi-round conversation, combined images (多图多轮对话，拼接图像)\n",
    "pixel_values1 = utils.load_image('./examples/examples_image1.jpg', max_num=12).to(torch.bfloat16).cuda()\n",
    "pixel_values2 = utils.load_image('./examples/examples_image2.jpg', max_num=12).to(torch.bfloat16).cuda()\n",
    "pixel_values = torch.cat((pixel_values1, pixel_values2), dim=0)\n",
    "\n",
    "question = '<image>\\nDescribe the two images in detail.'\n",
    "response, history = model.chat(tokenizer, pixel_values, question, generation_config,\n",
    "                               history=None, return_history=True)\n",
    "print(f'User: {question}\\nAssistant: {response}')\n",
    "\n",
    "question = 'What are the similarities and differences between these two images.'\n",
    "response, history = model.chat(tokenizer, pixel_values, question, generation_config,\n",
    "                               history=history, return_history=True)\n",
    "print(f'User: {question}\\nAssistant: {response}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b09d8fac-4606-4a85-8a30-174b43c8bec9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# multi-image multi-round conversation, separate images (多图多轮对话，独立图像)\n",
    "pixel_values1 = utils.load_image('./examples/examples_image1.jpg', max_num=12).to(torch.bfloat16).cuda()\n",
    "pixel_values2 = utils.load_image('./examples/examples_image2.jpg', max_num=12).to(torch.bfloat16).cuda()\n",
    "pixel_values = torch.cat((pixel_values1, pixel_values2), dim=0)\n",
    "num_patches_list = [pixel_values1.size(0), pixel_values2.size(0)]\n",
    "\n",
    "question = 'Image-1: <image>\\nImage-2: <image>\\nDescribe the two images in detail.'\n",
    "response, history = model.chat(tokenizer, pixel_values, question, generation_config,\n",
    "                               num_patches_list=num_patches_list,\n",
    "                               history=None, return_history=True)\n",
    "print(f'User: {question}\\nAssistant: {response}')\n",
    "\n",
    "question = 'What are the similarities and differences between these two images.'\n",
    "response, history = model.chat(tokenizer, pixel_values, question, generation_config,\n",
    "                               num_patches_list=num_patches_list,\n",
    "                               history=history, return_history=True)\n",
    "print(f'User: {question}\\nAssistant: {response}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b157f2af-9018-41e5-864f-984d4145ba1c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# batch inference, single image per sample (单图批处理)\n",
    "pixel_values1 = utils.load_image('./examples/examples_image1.jpg', max_num=12).to(torch.bfloat16).cuda()\n",
    "pixel_values2 = utils.load_image('./examples/examples_image2.jpg', max_num=12).to(torch.bfloat16).cuda()\n",
    "num_patches_list = [pixel_values1.size(0), pixel_values2.size(0)]\n",
    "pixel_values = torch.cat((pixel_values1, pixel_values2), dim=0)\n",
    "\n",
    "questions = ['<image>\\nDescribe the image in detail.'] * len(num_patches_list)\n",
    "responses = model.batch_chat(tokenizer, pixel_values,\n",
    "                             num_patches_list=num_patches_list,\n",
    "                             questions=questions,\n",
    "                             generation_config=generation_config)\n",
    "for question, response in zip(questions, responses):\n",
    "    print(f'User: {question}\\nAssistant: {response}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f193e3ac-baae-4500-baaf-517c4d8d1dec",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "video_path = './examples/red-panda.mp4'\n",
    "pixel_values, num_patches_list = utils.load_video(video_path, num_segments=8, max_num=1)\n",
    "pixel_values = pixel_values.to(torch.bfloat16).cuda()\n",
    "video_prefix = ''.join([f'Frame{i+1}: <image>\\n' for i in range(len(num_patches_list))])\n",
    "question = video_prefix + 'What is the red panda doing?'\n",
    "# Frame1: <image>\\nFrame2: <image>\\n...\\nFrame8: <image>\\n{question}\n",
    "response, history = model.chat(tokenizer, pixel_values, question, generation_config,\n",
    "                               num_patches_list=num_patches_list, history=None, return_history=True)\n",
    "print(f'User: {question}\\nAssistant: {response}')\n",
    "\n",
    "question = 'Describe this video in detail. Don\\'t repeat.'\n",
    "response, history = model.chat(tokenizer, pixel_values, question, generation_config,\n",
    "                               num_patches_list=num_patches_list, history=history, return_history=True)\n",
    "print(f'User: {question}\\nAssistant: {response}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "872a131d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_pytorch_p310",
   "language": "python",
   "name": "conda_pytorch_p310"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
